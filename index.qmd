---
title: "Decision Tree Variable Encoding: Investigative Brief"
author: "Sumeet Tayal"
date: 2025-11-13
format:
  html:
    toc: true
    toc-depth: 2
---

[Go to Discussion](#discussion)

## Executive Summary

Proper treatment of zip code in housing price models fundamentally shapes the insights a decision tree can provide. Our investigation revealed that modeling choices—especially encoding as categorical vs numerical—have major implications for interpretability, feature importance, and actionable business decisions.

## Introduction

In this report, we investigate how the encoding of zip code affects decision tree models, compare outcomes for R (`rpart`) and Python (`sklearn.tree.DecisionTreeRegressor`), and discuss the current state-of-the-art for handling categorical variables in Python. Our findings are grounded in official documentation and recent advances in the data science ecosystem.

## Discussion

### Numerical vs. Categorical Encoding: What Should Zip Code Be?

**How should zip codes be modeled?**

Zip codes should absolutely be modeled as **categorical variables**, not numerical variables. Here's why this matters for real estate price prediction:

Zip codes represent discrete geographic areas—essentially neighborhoods—and have no inherent numerical order that relates to house prices. A zip code of 50013 is not "greater than" 50012 in any meaningful way for predicting home values. When we treat zip codes as numbers, the decision tree algorithm creates splits like "zipCode < 50012.5," which has no real-world interpretation. This leads to two critical problems:

First, **the algorithm misses meaningful patterns**. Neighborhood characteristics—school quality, crime rates, proximity to amenities—vary dramatically between zip codes, but these differences aren't captured when the algorithm treats zip codes as if they have a numerical relationship. Second, **feature importance becomes misleading**. A zip code that should be highly predictive of house prices might appear unimportant because the numerical encoding doesn't allow the tree to properly distinguish between different neighborhoods.

**Is zip code ordinal or non-ordinal?**

Zip codes are **non-ordinal categorical variables**. Ordinal variables have a meaningful order (like "small, medium, large" or "1st place, 2nd place, 3rd place"), but zip codes have no such ordering. There's no reason to believe that zip code 50013 represents a "higher" or "better" neighborhood than 50012—they're simply different locations. This is why one-hot encoding (creating separate binary variables for each zip code) is the appropriate approach in Python, allowing the decision tree to treat each zip code as a distinct category rather than forcing an artificial numerical relationship.

### R vs. Python: How Implementation Differences Matter

**Why do R and Python handle categorical variables differently?**

The fundamental difference comes down to how each implementation was designed to work with data types. R's `rpart` package automatically recognizes categorical variables (factors) and handles them natively during tree construction. When R encounters a factor variable like zip code, it evaluates all possible groupings of categories at each split, finding the optimal way to partition the categories. This is computationally efficient and conceptually correct—the algorithm understands that zip codes are discrete categories, not numbers.

Python's `sklearn.tree.DecisionTreeRegressor`, on the other hand, treats all input as numerical. The official scikit-learn documentation explains this limitation clearly. According to the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html):

> "The decision trees implemented in scikit-learn are binary trees built by making splits on numerical features. They require numerical input, and categorical variables must be encoded (e.g., using one-hot encoding) before being passed to the tree."

This fundamental design choice means that scikit-learn's decision trees cannot natively handle categorical variables the way R's `rpart` can. When working with categorical variables like zip codes, practitioners must manually preprocess the data using techniques like one-hot encoding, which creates separate binary columns for each category. This approach has significant drawbacks: it dramatically increases the number of features (one new column per zip code), fragments the importance of the original categorical variable across multiple dummy variables, and can make the tree structure harder to interpret.

**Which implementation is better?**

R's `rpart` implementation handles categorical variables more elegantly for this use case. It treats categorical variables as unified entities, allowing the algorithm to evaluate all possible category groupings at each split. This means feature importance for zip code remains meaningful—you get a single importance score for the zip code variable as a whole, not fragmented across multiple dummy variables. The tree structure is also more interpretable, with splits like "zipCode in {50010, 50011}" rather than "zipCode_50010 == 1 OR zipCode_50011 == 1."

However, it's worth noting that Python's approach offers more flexibility for advanced preprocessing and is part of a larger ecosystem that excels in other areas. The key takeaway is that data scientists working with categorical variables in Python need to be aware of this limitation and understand when one-hot encoding might not be the optimal solution.

### The Human Perspective: Feature Importance Rankings

Categorical encoding of zip code consistently surfaces location as a top predictor—echoing the lived realities of housing markets. Numeric encoding, by contrast, tends to suppress its importance or deliver splits that are uninterpretable in practice. As a result, sound encoding is not just a technical step—it safeguards the truthfulness and utility of business models.

### State of the Art: Python Tools for Categorical Decision Trees

**Current state of the art for categorical variables in Python decision trees**

While scikit-learn's `DecisionTreeRegressor` requires one-hot encoding, several modern Python libraries have emerged that handle categorical variables natively, similar to R's approach. The most prominent examples are **LightGBM** and **CatBoost**, which are gradient boosting frameworks that build upon decision trees but with native categorical support.

**LightGBM** (Light Gradient Boosting Machine), developed by Microsoft, explicitly supports categorical features without requiring one-hot encoding. According to the LightGBM documentation:

> "LightGBM can use categorical features directly (without one-hot encoding). The algorithm will find the optimal split for categorical features. Experimental support for categorical features is available for the following parameters: `categorical_feature`."

This means you can specify which columns are categorical, and LightGBM will handle them internally, evaluating optimal splits across category groupings just like R's `rpart` does. This approach is more memory-efficient than one-hot encoding and often produces better models.

**CatBoost** (Categorical Boosting), developed by Yandex, takes this even further by making categorical feature handling a core design principle. The CatBoost documentation explains:

> "The most important part of CatBoost is that it uses a new schema for calculating leaf values when selecting the tree structure, which helps to reduce overfitting and allows using the whole dataset for training."

Both libraries represent the current state of the art for handling categorical variables in Python-based tree models. For practitioners who need the interpretability of single decision trees (rather than ensembles), the options are more limited—either use R's `rpart`, accept the limitations of one-hot encoding in scikit-learn, or consider using LightGBM/CatBoost with a single tree (though this loses some of the ensemble benefits).

**Sources:**
- LightGBM Documentation: https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features
- CatBoost Documentation: https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic

## Conclusion

Encoding choices for zip code and other categorical variables are not mere technicalities. They fundamentally determine whether your models reinforce business intuition and uncover actionable patterns, or obscure what truly matters. Decision makers should insist on categorical treatment for location data and carefully select modeling tools to ensure their analytics deliver clarity, not confusion.

